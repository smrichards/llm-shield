---
title: Providers
description: Configure your LLM providers
---

# Providers

PasteGuard supports two provider types: configured providers (`providers`) and local provider (`local`).

## Providers

Required for both modes. Any OpenAI-compatible endpoint works â€” cloud services (OpenAI, Azure, OpenRouter) or self-hosted (LiteLLM proxy, vLLM).

```yaml
providers:
  openai:
    base_url: https://api.openai.com/v1
    # api_key: ${OPENAI_API_KEY}  # Optional fallback
```

| Option | Description |
|--------|-------------|
| `base_url` | API endpoint (any OpenAI-compatible URL) |
| `api_key` | Optional. Used if client doesn't send Authorization header |

### Supported Endpoints

Any OpenAI-compatible API works:

```yaml
# OpenAI
providers:
  openai:
    base_url: https://api.openai.com/v1

# Azure OpenAI
providers:
  openai:
    base_url: https://your-resource.openai.azure.com/openai/v1

# OpenRouter
providers:
  openai:
    base_url: https://openrouter.ai/api/v1
    api_key: ${OPENROUTER_API_KEY}

# LiteLLM Proxy (self-hosted)
providers:
  openai:
    base_url: http://localhost:4000

# Together AI
providers:
  openai:
    base_url: https://api.together.xyz/v1

# Groq
providers:
  openai:
    base_url: https://api.groq.com/openai/v1
```

## Local Provider

Required for route mode only. Your local LLM for PII requests.

```yaml
local:
  type: ollama
  base_url: http://localhost:11434
  model: llama3.2
```

| Option | Description |
|--------|-------------|
| `type` | `ollama` or `openai` (for compatible servers) |
| `base_url` | Local LLM endpoint |
| `model` | Model to use for all PII requests |
| `api_key` | Only needed for OpenAI-compatible servers |

### Ollama

```yaml
local:
  type: ollama
  base_url: http://localhost:11434
  model: llama3.2
```

### vLLM

```yaml
local:
  type: openai
  base_url: http://localhost:8000/v1
  model: meta-llama/Llama-2-7b-chat-hf
```

### llama.cpp

```yaml
local:
  type: openai
  base_url: http://localhost:8080/v1
  model: local
```

### LocalAI

```yaml
local:
  type: openai
  base_url: http://localhost:8080/v1
  model: your-model
  api_key: ${LOCAL_API_KEY}  # if required
```

## API Key Handling

PasteGuard forwards your client's `Authorization` header to the configured provider. You can optionally set `api_key` in config as a fallback:

```yaml
providers:
  openai:
    base_url: https://api.openai.com/v1
    api_key: ${OPENAI_API_KEY}  # Used if client doesn't send auth
```
